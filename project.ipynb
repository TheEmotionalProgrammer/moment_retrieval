{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***SETUP:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UmQotmJMUrs",
        "outputId": "63f171cd-ae67-4549-80f8-f3ac3825cd24"
      },
      "outputs": [],
      "source": [
        "pip install python-terrier==0.10.0 nltk scikit-learn lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "##UNCOMMENT IF WORKING ON COLAB!\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "EYhewvEjL-7Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import pyterrier as pt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***DATASETS CREATION AND PREPROCESSING:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "#UNCOMMENT ONE OF THE FOLLOWING LINES TO USE EITHER THE TVR DATASET OR THE QVH DATASET\n",
        "\n",
        "dataset_choice = \"TVR\"\n",
        "# dataset_choice = \"QVH\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "W17JgCyFL-7Z"
      },
      "outputs": [],
      "source": [
        "# Paths to JSONL files. \n",
        "if dataset_choice == \"TVR\": \n",
        "    jsonl_train_path = 'text_data/tvr_train_release.jsonl'\n",
        "    jsonl_val_path = 'text_data/tvr_val_release.jsonl'\n",
        "    subs_path = 'text_data/tvqa_preprocessed_subtitles.jsonl'\n",
        "\n",
        "elif dataset_choice == \"QVH\":\n",
        "    jsonl_train_path = \"text_data_QVH/highlight_train_release.jsonl\"\n",
        "    subs_path = \"text_data_QVH/subs_train.jsonl\"\n",
        "\n",
        "##UNCOMMENT IF WORKING ON COLAB!\n",
        "# if dataset_choice == \"TVR\": \n",
        "#     jsonl_train_path = '/content/drive/MyDrive/IR/text_data/tvr_train_release.jsonl'\n",
        "#     jsonl_val_path = '/content/drive/MyDrive/IR/text_data/tvr_val_release.jsonl'\n",
        "#     subs_path = '/content/drive/MyDrive/IR/text_data/tvqa_preprocessed_subtitles.jsonl'\n",
        "\n",
        "# elif dataset_choice == \"QVH\":\n",
        "#     jsonl_train_path = \"/content/drive/MyDrive/IR/text_data_QVH/highlight_train_release.jsonl\"\n",
        "#     subs_path = \"/content/drive/MyDrive/IR/text_data_QVH/subs_train.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "L6VZMDuDL-7Z"
      },
      "outputs": [],
      "source": [
        "# Load subtitles into a dictionary for quick access\n",
        "subtitles_dict = {}\n",
        "if dataset_choice == \"TVR\":\n",
        "    with open(subs_path, 'r') as subs_file:\n",
        "        for line in subs_file:\n",
        "            sub_data = json.loads(line)\n",
        "            subtitles_dict[sub_data['vid_name']] = sub_data['sub']\n",
        "elif dataset_choice == \"QVH\":\n",
        "    with open(subs_path, 'r') as subs_file:\n",
        "        for line in subs_file:\n",
        "            sub_data = json.loads(line)\n",
        "            triple = sub_data['vid'].split(\"_\")\n",
        "            name = triple[0:-2]\n",
        "            #turn the list name into a string\n",
        "            name = \"\".join(name)\n",
        "            if name not in subtitles_dict:\n",
        "                subtitles_dict[name] = [(float(triple[-2]) + sub_data[\"relevant_windows\"][0][0], float(triple[-2]) + sub_data[\"relevant_windows\"][0][1], sub_data['query'])]\n",
        "            else:\n",
        "                subtitles_dict[name].append((float(triple[-2]) + sub_data[\"relevant_windows\"][0][0], float(triple[-2]) + sub_data[\"relevant_windows\"][0][1], sub_data['query']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "UkWQOjjYL-7Z"
      },
      "outputs": [],
      "source": [
        "# Function to find matching subtitles in TVR case\n",
        "def find_matching_subtitles(vid_name, ts_range, subtitles_dict):\n",
        "    matching_subs = []\n",
        "    if vid_name in subtitles_dict:\n",
        "        for subtitle in subtitles_dict[vid_name]:\n",
        "            if (ts_range[0] <= subtitle['start'] <= ts_range[1]) or (ts_range[0] <= subtitle['end'] <= ts_range[1]) or (subtitle['start'] <= ts_range[0] and subtitle['end'] >= ts_range[1]):\n",
        "                matching_subs.append(subtitle['text'])\n",
        "    return matching_subs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "HDx8ntjxL-7a"
      },
      "outputs": [],
      "source": [
        "def parse_jsonl_TVR(jsonl_path, split_type):\n",
        "    # Initialize empty lists for your data\n",
        "    queries_data = []\n",
        "    documents_data = []\n",
        "    query_rankings_data = []\n",
        "\n",
        "    with open(jsonl_path, 'r') as file:\n",
        "        for idx, line in enumerate(file):\n",
        "            data = json.loads(line)\n",
        "            # drop non text-based queries\n",
        "            if data['type'] not in ['t']: #add vt to include video-text queries\n",
        "                continue\n",
        "\n",
        "            # Find matching subtitles\n",
        "            matching_subs = find_matching_subtitles(data['vid_name'], data['ts'], subtitles_dict)\n",
        "\n",
        "            if matching_subs == []:\n",
        "                continue\n",
        "            \n",
        "            # Extract data for the Query Set DataFrame\n",
        "            queries_data.append({'qid': str(data['desc_id']), 'query': data['desc']})\n",
        "\n",
        "            # Extract data for the Documents Set DataFrame, including matching subtitles\n",
        "            documents_data.append({'docno': split_type + str(idx), 'vid_name': data['vid_name'], 'ts': data['ts'],\n",
        "                                'duration': data['duration'], 'type': data['type'], 'text': \"\".join(matching_subs)})\n",
        "\n",
        "            # Extract data for the Query Rankings DataFrame\n",
        "            query_rankings_data.append({'qid': str(data[\"desc_id\"]), 'query': data['desc'], 'docno': split_type + str(idx), 'rank': 1, 'score': 1.0})\n",
        "\n",
        "    return queries_data, documents_data, query_rankings_data\n",
        "\n",
        "def parse_jsonl_QVH(jsonl_path):\n",
        "    queries_data = []\n",
        "    documents_data = []\n",
        "    query_rankings_data = []\n",
        "    with open(jsonl_path, 'r') as file:\n",
        "        for idx,line in enumerate(file):\n",
        "\n",
        "            # Load the JSON object from the line\n",
        "            data = json.loads(line)\n",
        "\n",
        "            triple = data[\"vid\"].split(\"_\")\n",
        "            document_name = triple[0:-2]\n",
        "            document_name = \"\".join(document_name)\n",
        "            start_time = float(triple[-2])\n",
        "            end_time = float(triple[-1])\n",
        "\n",
        "            if document_name not in subtitles_dict:\n",
        "                #print(\"Document not found in subtitles: \", document_name)\n",
        "                continue\n",
        "\n",
        "            momentaneus_rank =[]\n",
        "            count = 0\n",
        "            for id,relevant_window in enumerate(data[\"relevant_windows\"]):\n",
        "                ts = [start_time+relevant_window[0], start_time+relevant_window[1]]\n",
        "                subs = [sub for sub in subtitles_dict[document_name] if sub[0] <= ts[1] and ts[0] <= sub[1]]\n",
        "                if len(subs) == 0:\n",
        "                    #print(\"No subtitles found for \", document_name, \" at time \", ts)\n",
        "                    continue\n",
        "                count += 1\n",
        "                documents_data.append({\"docno\" : str(idx) +\"_\"+str(ts[0]) + \"_\" + str(ts[1]), \"vid_name\" : document_name, \"ts\": ts, \"duration\": data[\"duration\"], \"text\": \"\".join([sub[2] for sub in subs])})\n",
        "                scores = [data[\"saliency_scores\"][i]  for i,clip_id in enumerate(data[\"relevant_clip_ids\"]) if clip_id*2 >= relevant_window[0] and clip_id*2 <= relevant_window[1]]\n",
        "                if len(scores) == 0:\n",
        "                    #print(\"No scores found for \", document_name, \" at time \", ts)\n",
        "                    continue\n",
        "                #each entry of scores is a triple of integers. Create a variable score which is the average of all the scores\n",
        "                score = 0 if len(scores) ==0 else sum(sum(scores[i]) for i in range(len(scores)))/(3*len(scores))\n",
        "\n",
        "                momentaneus_rank.append({\"qid\" : str(data[\"qid\"]), \"query\": data[\"query\"] , \"docno\" : str(idx) +\"_\"+str(ts[0]) + \"_\" + str(ts[1]), \"score\": score, \"rank\":1})\n",
        "            \n",
        "            if count == 0:\n",
        "                #print(\"No relevant windows found for \", document_name)\n",
        "                continue\n",
        "            #adjust the rank of the momentaneus_rank based on the score\n",
        "            momentaneus_rank = sorted(momentaneus_rank, key=lambda x: x[\"score\"], reverse=True)\n",
        "            for i in range(len(momentaneus_rank)):\n",
        "                momentaneus_rank[i][\"rank\"] = i+1\n",
        "            queries_data.append({\"qid\" : str(data[\"qid\"]), \"query\": data[\"query\"]})\n",
        "            query_rankings_data.extend(momentaneus_rank)\n",
        "\n",
        "    return queries_data, documents_data, query_rankings_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CREATION AND SPLIT OF THE DATASETS\n",
        "\n",
        "if dataset_choice == \"TVR\":\n",
        "    queries_data_train, documents_data_train, query_rankings_data_train = parse_jsonl_TVR(jsonl_train_path, \"t\") #the letter is just to avoid having the same docno for different documents\n",
        "    queries_data_val, documents_data_val, query_rankings_data_val = parse_jsonl_TVR(jsonl_val_path, \"v\")\n",
        "    #have to create a test set; to do it, extract a random 10% of the train set\n",
        "    random.seed(42)\n",
        "    query_rankings_data_test = random.sample(query_rankings_data_train, int(len(query_rankings_data_train)*0.1))\n",
        "    query_rankings_data_train = [query for query in query_rankings_data_train if query not in query_rankings_data_test]\n",
        "    queries_data_test = [query for query in queries_data_train if query[\"qid\"] in [query[\"qid\"] for query in query_rankings_data_test]]\n",
        "    queries_data_train = [query for query in queries_data_train if query not in queries_data_test]\n",
        "    documents_data_test = [doc for doc in documents_data_train if doc[\"docno\"] in [query[\"docno\"] for query in query_rankings_data_test]]\n",
        "    documents_data_train = [doc for doc in documents_data_train if doc not in documents_data_test]\n",
        "\n",
        "elif dataset_choice == \"QVH\":\n",
        "    queries_data_train, documents_data_train, query_rankings_data_train = parse_jsonl_QVH(jsonl_train_path)\n",
        "    #have to create a val set; to do it, extract a random 10% of the train set\n",
        "    random.seed(42)\n",
        "    query_rankings_data_val = random.sample(query_rankings_data_train, int(len(query_rankings_data_train)*0.1))\n",
        "    query_rankings_data_train = [query for query in query_rankings_data_train if query not in query_rankings_data_val]\n",
        "    queries_data_val = [query for query in queries_data_train if query[\"qid\"] in [query[\"qid\"] for query in query_rankings_data_val]]\n",
        "    queries_data_train = [query for query in queries_data_train if query not in queries_data_val]\n",
        "    documents_data_val = [doc for doc in documents_data_train if doc[\"docno\"] in [query[\"docno\"] for query in query_rankings_data_val]]\n",
        "    documents_data_train = [doc for doc in documents_data_train if doc not in documents_data_val]\n",
        "    #have to create a test set; to do it, extract a random 10% of the remaining train set\n",
        "    random.seed(42)\n",
        "    query_rankings_data_test = random.sample(query_rankings_data_train, int(len(query_rankings_data_train)*0.1))\n",
        "    query_rankings_data_train = [query for query in query_rankings_data_train if query not in query_rankings_data_test]\n",
        "    queries_data_test = [query for query in queries_data_train if query[\"qid\"] in [query[\"qid\"] for query in query_rankings_data_test]]\n",
        "    queries_data_train = [query for query in queries_data_train if query not in queries_data_test]\n",
        "    documents_data_test = [doc for doc in documents_data_train if doc[\"docno\"] in [query[\"docno\"] for query in query_rankings_data_test]]\n",
        "    documents_data_train = [doc for doc in documents_data_train if doc not in documents_data_test]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTiDgmaqL-7a",
        "outputId": "cdd8b431-1446-4f12-e41b-8131683db4aa"
      },
      "outputs": [],
      "source": [
        "# Create DataFrames for the Query Set, Documents Set, and Query Rankings\n",
        "queries_train_df = pd.DataFrame(queries_data_train)\n",
        "documents_train_df = pd.DataFrame(documents_data_train)\n",
        "query_rankings_data_train_df = pd.DataFrame(query_rankings_data_train)\n",
        "\n",
        "queries_val_df = pd.DataFrame(queries_data_val)\n",
        "documents_val_df = pd.DataFrame(documents_data_val)\n",
        "query_rankings_data_val_df = pd.DataFrame(query_rankings_data_val)\n",
        "\n",
        "queries_test_df = pd.DataFrame(queries_data_test)\n",
        "documents_test_df = pd.DataFrame(documents_data_test)\n",
        "query_rankings_data_test_df = pd.DataFrame(query_rankings_data_test)\n",
        "\n",
        "#Sometimes it is useful to have all the qrels data in a single dataframe\n",
        "q_rels = pd.concat([pd.DataFrame(query_rankings_data_train), pd.DataFrame(query_rankings_data_val), pd.DataFrame(query_rankings_data_test)]).reset_index(drop=True)\n",
        "\n",
        "#print length of the dataframes\n",
        "print(\"Train set:\")\n",
        "print(\"Queries: \", len(queries_train_df))\n",
        "print(\"Documents: \", len(documents_train_df))\n",
        "\n",
        "print(\"Val set:\")\n",
        "print(\"Queries: \", len(queries_val_df))\n",
        "print(\"Documents: \", len(documents_val_df))\n",
        "\n",
        "print(\"Test set:\")\n",
        "print(\"Queries: \", len(queries_test_df))\n",
        "print(\"Documents: \", len(documents_test_df))\n",
        "\n",
        "print(\"Query Rankings: \", len(query_rankings_data_train))\n",
        "print(\"Query Rankings: \", len(query_rankings_data_val))\n",
        "print(\"Query Rankings: \", len(query_rankings_data_test))\n",
        "\n",
        "print(\"Query Rankings: \", len(q_rels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***INITIALIZATION AND INDEXING:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-SsgPJwVMXw",
        "outputId": "d3e0760a-6892-4d05-f38c-821e9364b978"
      },
      "outputs": [],
      "source": [
        "if not pt.started():\n",
        "    pt.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Xx55BjIGfNNa"
      },
      "outputs": [],
      "source": [
        "# Create an index for the documents\n",
        "\n",
        "indexer = pt.IterDictIndexer(\n",
        "    \"./index_path/\",\n",
        "    meta={\n",
        "        \"docno\": 64,\n",
        "        \"vid_name\": 64,\n",
        "        \"text\": 131072,\n",
        "    },\n",
        "    stemmer=\"porter\",\n",
        "    stopwords=\"terrier\",\n",
        "    overwrite=True,\n",
        "    # type=pt.index.IndexingType.MEMORY,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1W-_BFDjpqS"
      },
      "outputs": [],
      "source": [
        "joint_documents_set_df = pd.concat([documents_train_df, documents_val_df, documents_test_df])\n",
        "\n",
        "print(\"Length: \", len(joint_documents_set_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_PnpogfjpqS",
        "outputId": "2b494455-4836-4157-faf8-d1da376177dc"
      },
      "outputs": [],
      "source": [
        "indexed = indexer.index(\n",
        "    joint_documents_set_df.to_dict(orient=\"records\")\n",
        ")\n",
        "\n",
        "#NOTE: when it says 'empy documents added', it is a warning referred to \n",
        "#      subtitles only made of stopwords. It is not a problem for the indexing\n",
        "#      but of course these subtitles will not be useful for the retrieval\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***FIRST STAGE RETRIEVERS:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "9xuAPswuVQMe"
      },
      "outputs": [],
      "source": [
        "#These are some examples, not necessarily the best ones. Experiment with different models \n",
        "\n",
        "# Initialize BatchRetrieve with the created index and specify BM25 as the weighting model\n",
        "first_stage_bm25 = pt.BatchRetrieve(\n",
        "    indexed,\n",
        "    wmodel=\"BM25\",\n",
        "    num_results=3,\n",
        "    metadata=[\"docno\", \"vid_name\", \"text\"]\n",
        ")\n",
        "\n",
        "# Initialize BatchRetrieve with the created index and specify LemurTF_IDF as the weighting model\n",
        "first_stage_lemurtfidf = pt.BatchRetrieve(\n",
        "    indexed,\n",
        "    wmodel=\"LemurTF_IDF\",\n",
        "    num_results=3,\n",
        "    metadata=[\"docno\", \"vid_name\", \"text\"]\n",
        ")\n",
        "\n",
        "# Initialize BatchRetrieve with the created index and specify Hiemstra_LM as the weighting model\n",
        "first_stage_hiemstra_lm = pt.BatchRetrieve(\n",
        "    indexed,\n",
        "    wmodel=\"Hiemstra_LM\",\n",
        "    num_results=3,\n",
        "    metadata=[\"docno\", \"vid_name\", \"text\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***FSR SCORES AS FEATURES:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "QTBZtHYLy_fH"
      },
      "outputs": [],
      "source": [
        "#We create features for the second stage using the first stage retrievers\n",
        "\n",
        "#TF-IDF based features\n",
        "lemur_tf_idf_retriever = pt.BatchRetrieve(indexed, wmodel=\"LemurTF_IDF\")\n",
        "bm25_retriever = pt.BatchRetrieve(indexed, wmodel=\"BM25\")\n",
        "tf_idf_retriever = pt.BatchRetrieve(indexed, wmodel=\"TF_IDF\")\n",
        "\n",
        "#Language model based features\n",
        "hiem_retriever = pt.BatchRetrieve(indexed, wmodel=\"Hiemstra_LM\")\n",
        "dirichlet_retriever = pt.BatchRetrieve(indexed, wmodel=\"DirichletLM\")\n",
        "\n",
        "#Divergence from randomness based features\n",
        "pl2_retriever = pt.BatchRetrieve(indexed, wmodel=\"PL2\")\n",
        "dph_retriever = pt.BatchRetrieve(indexed, wmodel=\"DPH\")\n",
        "dlh_retriever = pt.BatchRetrieve(indexed, wmodel=\"DLH\")\n",
        "\n",
        "#Can add more!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "AYrqwBx1vVn_"
      },
      "outputs": [],
      "source": [
        "# PIPELINES WITH FSR AND FEATURES\n",
        "\n",
        "#Can test many possibilities, i just put some examples here\n",
        "\n",
        "bm25_pipeline = ~first_stage_bm25 >> (\n",
        "   pl2_retriever ** dph_retriever ** tf_idf_retriever  \n",
        ")\n",
        "\n",
        "\n",
        "lemurtf_idf_pipeline = ~first_stage_lemurtfidf >> (\n",
        "    pl2_retriever ** dph_retriever ** hiem_retriever\n",
        ")\n",
        "\n",
        "hiem_lm_pipeline = ~hiem_retriever >> (\n",
        "    pl2_retriever ** dph_retriever ** tf_idf_retriever\n",
        ")\n",
        "\n",
        "#NOTE: i believe we should not use the same first stage retriever for features AND for the first stage retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "QE1WrfC8zJjm"
      },
      "outputs": [],
      "source": [
        "# Prepare the queries for the pipeline, remove special characters and extra spaces\n",
        "\n",
        "prepared_trainqueries = queries_train_df\n",
        "prepared_trainqueries['query'] = prepared_trainqueries['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
        "prepared_trainqueries['query'] = prepared_trainqueries['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "prepared_trainqueries['query'] = prepared_trainqueries['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "prepared_train_qrels = query_rankings_data_train_df\n",
        "prepared_train_qrels['query'] = prepared_train_qrels['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
        "prepared_train_qrels['query'] = prepared_train_qrels['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "prepared_train_qrels['query'] = prepared_train_qrels['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "prepared_train_qrels['label'] = prepared_train_qrels['score']\n",
        "\n",
        "prepared_train_qrels['label'] = prepared_train_qrels['label'].astype(int)\n",
        "\n",
        "prepared_val_qrels = query_rankings_data_val_df\n",
        "prepared_val_qrels['query'] = prepared_val_qrels['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
        "prepared_val_qrels['query'] = prepared_val_qrels['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "prepared_val_qrels['query'] = prepared_val_qrels['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "prepared_val_qrels['label'] = prepared_val_qrels['score']\n",
        "\n",
        "prepared_val_qrels['label'] = prepared_val_qrels['label'].astype(int)\n",
        "\n",
        "prepared_test_qrels = query_rankings_data_test_df\n",
        "prepared_test_qrels['query'] = prepared_test_qrels['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
        "prepared_test_qrels['query'] = prepared_test_qrels['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "prepared_test_qrels['query'] = prepared_test_qrels['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "prepared_test_qrels['label'] = prepared_test_qrels['score']\n",
        "\n",
        "prepared_test_qrels['label'] = prepared_test_qrels['label'].astype(int)\n",
        "\n",
        "prepared_qrels = q_rels\n",
        "prepared_qrels['query'] = prepared_qrels['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
        "prepared_qrels['query'] = prepared_qrels['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "prepared_qrels['query'] = prepared_qrels['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "prepared_qrels['label'] = prepared_qrels['score']\n",
        "\n",
        "prepared_qrels['label'] = prepared_qrels['label'].astype(int)\n",
        "\n",
        "prepared_valqueries = queries_val_df\n",
        "prepared_valqueries['query'] = prepared_valqueries['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
        "prepared_valqueries['query'] = prepared_valqueries['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "\n",
        "prepared_valqueries['query'] = prepared_valqueries['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "#test set\n",
        "prepared_testqueries = queries_test_df\n",
        "prepared_testqueries['query'] = prepared_testqueries['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
        "prepared_testqueries['query'] = prepared_testqueries['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "prepared_testqueries['query'] = prepared_testqueries['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***LEARNING TO RANK:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "import xgboost as xgb\n",
        "import fastrank\n",
        "\n",
        "\n",
        "index = pt.IndexFactory.of(\"./index_path\")\n",
        "\n",
        "#Uncomment the pipelines you want to use\n",
        "\n",
        "fsr_pipelines = [\n",
        "    {\n",
        "        'pipe': bm25_pipeline,\n",
        "        'name': 'BM25'\n",
        "    },\n",
        "    {\n",
        "        'pipe': lemurtf_idf_pipeline,\n",
        "        'name': 'LemurTF_IDF'\n",
        "    },\n",
        "    # {\n",
        "    #     'pipe': hiem_lm_pipeline,\n",
        "    #     'name': 'Hiemstra_LM'\n",
        "    # }\n",
        "]\n",
        "\n",
        "#Uncomment the models you want to use\n",
        "\n",
        "learned_models = [\n",
        "    # {\n",
        "    #     'model': SVR(),\n",
        "    #     'form': 'reg',\n",
        "    #     'name': 'SVR',\n",
        "    # },\n",
        "    {\n",
        "        'model': xgb.XGBRanker(tree_method=\"hist\", objective=\"rank:ndcg\"),\n",
        "        'form': 'ltr',\n",
        "        'name': 'XGBoost (NDCG)',\n",
        "     },\n",
        "    # {\n",
        "    #     'model': xgb.XGBRanker(tree_method=\"hist\", objective=\"rank:map\"),\n",
        "    #     'form': 'ltr',\n",
        "    #     'name': 'XGBoost (MAP)',\n",
        "    # },\n",
        "#     {\n",
        "#         'model': fastrank.TrainRequest.coordinate_ascent(),\n",
        "#         'form': 'fastrank',\n",
        "#         'name': 'FastRank Coordinate Ascent',\n",
        "#     },\n",
        "#    {\n",
        "#         'model': fastrank.TrainRequest.random_forest(),\n",
        "#         'form': 'fastrank',\n",
        "#         'name': 'FastRank Random Forest',\n",
        "#     },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***TRAINING:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Put the names of the first stage retrievers you are using in the list\n",
        "\n",
        "trained_models = [first_stage_bm25, first_stage_lemurtfidf]\n",
        "names = ['BM25','LemurTF_IDF']\n",
        "\n",
        "for fsr in fsr_pipelines:\n",
        "    for model in learned_models:\n",
        "        names.append(f\"{fsr['name']} >> {model['name']}\")\n",
        "        print(names[-1])\n",
        "        pipe = fsr['pipe'] >> pt.ltr.apply_learned_model(model['model'], form=model['form'])\n",
        "        pipe.fit(\n",
        "            prepared_trainqueries,\n",
        "            prepared_train_qrels,\n",
        "            prepared_valqueries,\n",
        "            prepared_val_qrels\n",
        "        )\n",
        "        trained_models.append(pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***EXPERIMENTS AND RESULTS:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Results on the test set\n",
        "\n",
        "#NOTE: These are the only ones that really matter, but it could be useful \n",
        "#      to compare them with the ones on the train/val set to see if there is overfitting\n",
        "\n",
        "from pyterrier.measures import nDCG, RR, MAP\n",
        "\n",
        "pt.Experiment(\n",
        "    trained_models,\n",
        "    prepared_testqueries,\n",
        "    prepared_test_qrels,\n",
        "    names=names,\n",
        "    eval_metrics=[nDCG @ 1, RR @ 1, nDCG @ 3, RR @ 3, nDCG @ 5, RR @ 5, nDCG @ 10, RR @ 10, MAP],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Results on the train set\n",
        "\n",
        "pt.Experiment(\n",
        "    trained_models,\n",
        "    prepared_trainqueries,\n",
        "    prepared_train_qrels,\n",
        "    names=names,\n",
        "    eval_metrics=[nDCG @ 1, RR @ 1, nDCG @ 3, RR @ 3, nDCG @ 5, RR @ 5, nDCG @ 10, RR @ 10, MAP],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Results on the val set\n",
        "\n",
        "pt.Experiment(\n",
        "    trained_models,\n",
        "    prepared_valqueries,\n",
        "    prepared_val_qrels,\n",
        "    names=names,\n",
        "    eval_metrics=[nDCG @ 1, RR @ 1, nDCG @ 3, RR @ 3, nDCG @ 5, RR @ 5, nDCG @ 10, RR @ 10, MAP],\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
