{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdYzrLhxMTlW",
        "outputId": "f098c2dd-15fa-49fa-8a3e-6bd614661a65"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UmQotmJMUrs",
        "outputId": "63f171cd-ae67-4549-80f8-f3ac3825cd24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-terrier==0.10.0 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (0.10.0)\n",
            "Requirement already satisfied: nltk in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (3.6.5)\n",
            "Requirement already satisfied: scikit-learn in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (0.24.2)\n",
            "Requirement already satisfied: lightgbm in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (4.3.0)\n",
            "Requirement already satisfied: numpy in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (1.22.4)\n",
            "Requirement already satisfied: pandas in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (2.1.3)\n",
            "Requirement already satisfied: wget in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (3.2)\n",
            "Requirement already satisfied: tqdm in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (4.62.3)\n",
            "Requirement already satisfied: pyjnius>=1.4.2 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (1.6.1)\n",
            "Requirement already satisfied: matchpy in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (0.5.5)\n",
            "Requirement already satisfied: deprecated in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (1.2.14)\n",
            "Requirement already satisfied: chest in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (0.2.3)\n",
            "Requirement already satisfied: scipy in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (1.7.1)\n",
            "Requirement already satisfied: requests in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (2.31.0)\n",
            "Requirement already satisfied: joblib in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (1.1.0)\n",
            "Requirement already satisfied: nptyping==1.4.4 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (1.4.4)\n",
            "Requirement already satisfied: more-itertools in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (8.10.0)\n",
            "Requirement already satisfied: ir-datasets>=0.3.2 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (0.5.6)\n",
            "Requirement already satisfied: jinja2 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (2.11.3)\n",
            "Requirement already satisfied: statsmodels in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (0.12.2)\n",
            "Requirement already satisfied: ir-measures>=0.3.1 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (0.3.3)\n",
            "Requirement already satisfied: dill in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (0.3.6)\n",
            "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from python-terrier==0.10.0) (0.5.6)\n",
            "Requirement already satisfied: typish>=1.7.0 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from nptyping==1.4.4->python-terrier==0.10.0) (1.9.3)\n",
            "Requirement already satisfied: click in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2021.8.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (4.10.0)\n",
            "Requirement already satisfied: inscriptis>=2.2.0 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (2.4.0.1)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (5.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (6.0)\n",
            "Requirement already satisfied: trec-car-tools>=2.5.4 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (2.6)\n",
            "Requirement already satisfied: lz4>=3.1.10 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (4.3.3)\n",
            "Requirement already satisfied: warc3-wet>=0.2.3 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.2.3)\n",
            "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.2.5)\n",
            "Requirement already satisfied: zlib-state>=0.1.3 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.1.6)\n",
            "Requirement already satisfied: ijson>=3.1.3 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (3.2.3)\n",
            "Requirement already satisfied: pyautocorpus>=0.1.1 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.1.12)\n",
            "Requirement already satisfied: unlzw3>=0.2.1 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.2.2)\n",
            "Requirement already satisfied: cwl-eval>=1.0.10 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from ir-measures>=0.3.1->python-terrier==0.10.0) (1.0.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from requests->python-terrier==0.10.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from requests->python-terrier==0.10.0) (3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from requests->python-terrier==0.10.0) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from requests->python-terrier==0.10.0) (2021.10.8)\n",
            "Requirement already satisfied: heapdict in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from chest->python-terrier==0.10.0) (1.0.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from deprecated->python-terrier==0.10.0) (1.12.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from jinja2->python-terrier==0.10.0) (1.1.1)\n",
            "Requirement already satisfied: multiset<3.0,>=2.0 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from matchpy->python-terrier==0.10.0) (2.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from pandas->python-terrier==0.10.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from pandas->python-terrier==0.10.0) (2021.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from pandas->python-terrier==0.10.0) (2023.3)\n",
            "Requirement already satisfied: patsy>=0.5 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from statsmodels->python-terrier==0.10.0) (0.5.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier==0.10.0) (2.2.1)\n",
            "Requirement already satisfied: six in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from patsy>=0.5->statsmodels->python-terrier==0.10.0) (1.16.0)\n",
            "Requirement already satisfied: cbor>=1.0.0 in /Users/isidorotamassia/opt/anaconda3/lib/python3.9/site-packages (from trec-car-tools>=2.5.4->ir-datasets>=0.3.2->python-terrier==0.10.0) (1.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install python-terrier==0.10.0 nltk scikit-learn lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "EYhewvEjL-7Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import pyterrier as pt\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "#UNCOMMENT THE FOLLOWING LINE TO USE EITHER THE TVR DATASET OR THE QVH DATASET\n",
        "\n",
        "dataset_choice = \"TVR\"\n",
        "# dataset_choice = \"QVH\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "UruQwRMsOrNj"
      },
      "outputs": [],
      "source": [
        "# Paths to JSONL files on Colab\n",
        "# jsonl_train_path = '/content/drive/MyDrive/IR/text_data/tvr_train_release.jsonl'\n",
        "# jsonl_val_path = '/content/drive/MyDrive/IR/text_data/tvr_val_release.jsonl'\n",
        "# subs_path = '/content/drive/MyDrive/IR/text_data/tvqa_preprocessed_subtitles.jsonl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "W17JgCyFL-7Z"
      },
      "outputs": [],
      "source": [
        "# Paths to JSONL files\n",
        "if dataset_choice == \"TVR\": \n",
        "    jsonl_train_path = 'text_data/tvr_train_release.jsonl'\n",
        "    jsonl_val_path = 'text_data/tvr_val_release.jsonl'\n",
        "    subs_path = 'text_data/tvqa_preprocessed_subtitles.jsonl'\n",
        "\n",
        "elif dataset_choice == \"QVH\":\n",
        "    jsonl_train_path = \"text_data_QVH/highlight_train_release.jsonl\"\n",
        "    subs_path = \"text_data_QVH/subs_train.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "L6VZMDuDL-7Z"
      },
      "outputs": [],
      "source": [
        "# Load subtitles into a dictionary for quick access\n",
        "subtitles_dict = {}\n",
        "if dataset_choice == \"TVR\":\n",
        "    with open(subs_path, 'r') as subs_file:\n",
        "        for line in subs_file:\n",
        "            sub_data = json.loads(line)\n",
        "            subtitles_dict[sub_data['vid_name']] = sub_data['sub']\n",
        "elif dataset_choice == \"QVH\":\n",
        "    with open(subs_path, 'r') as subs_file:\n",
        "        for line in subs_file:\n",
        "            sub_data = json.loads(line)\n",
        "            triple = sub_data['vid'].split(\"_\")\n",
        "            name = triple[0:-2]\n",
        "            #turn the list name into a string\n",
        "            name = \"\".join(name)\n",
        "            if name not in subtitles_dict:\n",
        "                subtitles_dict[name] = [(float(triple[-2]) + sub_data[\"relevant_windows\"][0][0], float(triple[-2]) + sub_data[\"relevant_windows\"][0][1], sub_data['query'])]\n",
        "            else:\n",
        "                subtitles_dict[name].append((float(triple[-2]) + sub_data[\"relevant_windows\"][0][0], float(triple[-2]) + sub_data[\"relevant_windows\"][0][1], sub_data['query']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "UkWQOjjYL-7Z"
      },
      "outputs": [],
      "source": [
        "# Function to find matching subtitles in TVR case\n",
        "def find_matching_subtitles(vid_name, ts_range, subtitles_dict):\n",
        "    matching_subs = []\n",
        "    if vid_name in subtitles_dict:\n",
        "        for subtitle in subtitles_dict[vid_name]:\n",
        "            if (ts_range[0] <= subtitle['start'] <= ts_range[1]) or (ts_range[0] <= subtitle['end'] <= ts_range[1]) or (subtitle['start'] <= ts_range[0] and subtitle['end'] >= ts_range[1]):\n",
        "                matching_subs.append(subtitle['text'])\n",
        "    return matching_subs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "HDx8ntjxL-7a"
      },
      "outputs": [],
      "source": [
        "def parse_jsonl_TVR(jsonl_path, split_type):\n",
        "    # Initialize empty lists for your data\n",
        "    queries_data = []\n",
        "    documents_data = []\n",
        "    query_rankings_data = []\n",
        "\n",
        "    with open(jsonl_path, 'r') as file:\n",
        "        for idx, line in enumerate(file):\n",
        "            data = json.loads(line)\n",
        "            # drop non text-based queries\n",
        "            if data['type'] not in ['t']:\n",
        "                continue\n",
        "\n",
        "            # Find matching subtitles\n",
        "            matching_subs = find_matching_subtitles(data['vid_name'], data['ts'], subtitles_dict)\n",
        "\n",
        "            if matching_subs == []:\n",
        "                continue\n",
        "            \n",
        "            # Extract data for the Query Set DataFrame\n",
        "            queries_data.append({'qid': str(data['desc_id']), 'query': data['desc']})\n",
        "\n",
        "            # Extract data for the Documents Set DataFrame, including matching subtitles\n",
        "            documents_data.append({'docno': split_type + str(idx), 'vid_name': data['vid_name'], 'ts': data['ts'],\n",
        "                                'duration': data['duration'], 'type': data['type'], 'text': \"\".join(matching_subs)})\n",
        "\n",
        "            # Extract data for the Query Rankings DataFrame\n",
        "            query_rankings_data.append({'qid': str(data[\"desc_id\"]), 'query': data['desc'], 'docno': split_type + str(idx), 'rank': 1, 'score': 1.0})\n",
        "\n",
        "    return queries_data, documents_data, query_rankings_data\n",
        "\n",
        "def parse_jsonl_QVH(jsonl_path):\n",
        "    queries_data = []\n",
        "    documents_data = []\n",
        "    query_rankings_data = []\n",
        "    with open(jsonl_path, 'r') as file:\n",
        "        for idx,line in enumerate(file):\n",
        "\n",
        "            # Load the JSON object from the line\n",
        "            data = json.loads(line)\n",
        "\n",
        "            triple = data[\"vid\"].split(\"_\")\n",
        "            document_name = triple[0:-2]\n",
        "            document_name = \"\".join(document_name)\n",
        "            start_time = float(triple[-2])\n",
        "            end_time = float(triple[-1])\n",
        "\n",
        "            if document_name not in subtitles_dict:\n",
        "                #print(\"Document not found in subtitles: \", document_name)\n",
        "                continue\n",
        "\n",
        "            momentaneus_rank =[]\n",
        "            count = 0\n",
        "            for id,relevant_window in enumerate(data[\"relevant_windows\"]):\n",
        "                ts = [start_time+relevant_window[0], start_time+relevant_window[1]]\n",
        "                subs = [sub for sub in subtitles_dict[document_name] if sub[0] <= ts[1] and ts[0] <= sub[1]]\n",
        "                if len(subs) == 0:\n",
        "                    #print(\"No subtitles found for \", document_name, \" at time \", ts)\n",
        "                    continue\n",
        "                count += 1\n",
        "                documents_data.append({\"docno\" : str(idx) +\"_\"+str(ts[0]) + \"_\" + str(ts[1]), \"vid_name\" : document_name, \"ts\": ts, \"duration\": data[\"duration\"], \"text\": \"\".join([sub[2] for sub in subs])})\n",
        "                scores = [data[\"saliency_scores\"][i]  for i,clip_id in enumerate(data[\"relevant_clip_ids\"]) if clip_id*2 >= relevant_window[0] and clip_id*2 <= relevant_window[1]]\n",
        "                if len(scores) == 0:\n",
        "                    #print(\"No scores found for \", document_name, \" at time \", ts)\n",
        "                    continue\n",
        "                #each entry of scores is a triple of integers. Create a variable score which is the average of all the scores\n",
        "                score = 0 if len(scores) ==0 else sum(sum(scores[i]) for i in range(len(scores)))/(3*len(scores))\n",
        "\n",
        "                momentaneus_rank.append({\"qid\" : str(data[\"qid\"]), \"query\": data[\"query\"] , \"docno\" : str(idx) +\"_\"+str(ts[0]) + \"_\" + str(ts[1]), \"score\": score, \"rank\":1})\n",
        "            \n",
        "            if count == 0:\n",
        "                #print(\"No relevant windows found for \", document_name)\n",
        "                continue\n",
        "            #adjust the rank of the momentaneus_rank based on the score\n",
        "            momentaneus_rank = sorted(momentaneus_rank, key=lambda x: x[\"score\"], reverse=True)\n",
        "            for i in range(len(momentaneus_rank)):\n",
        "                momentaneus_rank[i][\"rank\"] = i+1\n",
        "            queries_data.append({\"qid\" : str(data[\"qid\"]), \"query\": data[\"query\"]})\n",
        "            query_rankings_data.extend(momentaneus_rank)\n",
        "\n",
        "    return queries_data, documents_data, query_rankings_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if dataset_choice == \"TVR\":\n",
        "    queries_data_train, documents_data_train, query_rankings_data_train = parse_jsonl_TVR(jsonl_train_path, \"t\")\n",
        "    queries_data_val, documents_data_val, query_rankings_data_val = parse_jsonl_TVR(jsonl_val_path, \"v\")\n",
        "    #have to create a test set; to do it, extract a random 10% of the train set\n",
        "    random.seed(42)\n",
        "    query_rankings_data_test = random.sample(query_rankings_data_train, int(len(query_rankings_data_train)*0.1))\n",
        "    query_rankings_data_train = [query for query in query_rankings_data_train if query not in query_rankings_data_test]\n",
        "    queries_data_test = [query for query in queries_data_train if query[\"qid\"] in [query[\"qid\"] for query in query_rankings_data_test]]\n",
        "    queries_data_train = [query for query in queries_data_train if query not in queries_data_test]\n",
        "    documents_data_test = [doc for doc in documents_data_train if doc[\"docno\"] in [query[\"docno\"] for query in query_rankings_data_test]]\n",
        "    documents_data_train = [doc for doc in documents_data_train if doc not in documents_data_test]\n",
        "\n",
        "elif dataset_choice == \"QVH\":\n",
        "    queries_data_train, documents_data_train, query_rankings_data_train = parse_jsonl_QVH(jsonl_train_path)\n",
        "    #have to create a val set; to do it, extract a random 10% of the train set\n",
        "    random.seed(42)\n",
        "    query_rankings_data_val = random.sample(query_rankings_data_train, int(len(query_rankings_data_train)*0.1))\n",
        "    query_rankings_data_train = [query for query in query_rankings_data_train if query not in query_rankings_data_val]\n",
        "    queries_data_val = [query for query in queries_data_train if query[\"qid\"] in [query[\"qid\"] for query in query_rankings_data_val]]\n",
        "    queries_data_train = [query for query in queries_data_train if query not in queries_data_val]\n",
        "    documents_data_val = [doc for doc in documents_data_train if doc[\"docno\"] in [query[\"docno\"] for query in query_rankings_data_val]]\n",
        "    documents_data_train = [doc for doc in documents_data_train if doc not in documents_data_val]\n",
        "    #have to create a test set; to do it, extract a random 10% of the train set\n",
        "    random.seed(42)\n",
        "    query_rankings_data_test = random.sample(query_rankings_data_train, int(len(query_rankings_data_train)*0.1))\n",
        "    query_rankings_data_train = [query for query in query_rankings_data_train if query not in query_rankings_data_test]\n",
        "    queries_data_test = [query for query in queries_data_train if query[\"qid\"] in [query[\"qid\"] for query in query_rankings_data_test]]\n",
        "    queries_data_train = [query for query in queries_data_train if query not in queries_data_test]\n",
        "    documents_data_test = [doc for doc in documents_data_train if doc[\"docno\"] in [query[\"docno\"] for query in query_rankings_data_test]]\n",
        "    documents_data_train = [doc for doc in documents_data_train if doc not in documents_data_test]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTiDgmaqL-7a",
        "outputId": "cdd8b431-1446-4f12-e41b-8131683db4aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set:\n",
            "Queries:  7110\n",
            "Documents:  7110\n",
            "Val set:\n",
            "Queries:  957\n",
            "Documents:  957\n",
            "Test set:\n",
            "Queries:  790\n",
            "Documents:  790\n",
            "Query Rankings:  8857\n"
          ]
        }
      ],
      "source": [
        "# Create DataFrames for the Query Set, Documents Set, and Query Rankings\n",
        "queries_train_df = pd.DataFrame(queries_data_train)\n",
        "documents_train_df = pd.DataFrame(documents_data_train)\n",
        "\n",
        "queries_val_df = pd.DataFrame(queries_data_val)\n",
        "documents_val_df = pd.DataFrame(documents_data_val)\n",
        "\n",
        "queries_test_df = pd.DataFrame(queries_data_test)\n",
        "documents_test_df = pd.DataFrame(documents_data_test)\n",
        "\n",
        "q_rels = pd.concat([pd.DataFrame(query_rankings_data_train), pd.DataFrame(query_rankings_data_val), pd.DataFrame(query_rankings_data_test)]).reset_index(drop=True)\n",
        "\n",
        "#print length of the dataframes\n",
        "print(\"Train set:\")\n",
        "print(\"Queries: \", len(queries_train_df))\n",
        "print(\"Documents: \", len(documents_train_df))\n",
        "\n",
        "print(\"Val set:\")\n",
        "print(\"Queries: \", len(queries_val_df))\n",
        "print(\"Documents: \", len(documents_val_df))\n",
        "\n",
        "print(\"Test set:\")\n",
        "print(\"Queries: \", len(queries_test_df))\n",
        "print(\"Documents: \", len(documents_test_df))\n",
        "\n",
        "print(\"Query Rankings: \", len(q_rels))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCaV5vcXL-7a"
      },
      "source": [
        "### First Stage Retrieval [TODO: BOX]\n",
        "The following part of the code will define three different first stage retrieval pipelines as an input for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-SsgPJwVMXw",
        "outputId": "d3e0760a-6892-4d05-f38c-821e9364b978"
      },
      "outputs": [],
      "source": [
        "if not pt.started():\n",
        "    pt.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Xx55BjIGfNNa"
      },
      "outputs": [],
      "source": [
        "# Create an index\n",
        "from pathlib import Path\n",
        "\n",
        "indexer = pt.IterDictIndexer(\n",
        "    \"./index_path/\",\n",
        "    meta={\n",
        "        \"docno\": 64,\n",
        "        \"vid_name\": 64,\n",
        "        \"text\": 131072,\n",
        "    },\n",
        "    stemmer=\"porter\",\n",
        "    stopwords=\"terrier\",\n",
        "    overwrite=True,\n",
        "    # type=pt.index.IndexingType.MEMORY,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "T1W-_BFDjpqS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length:  8857\n"
          ]
        }
      ],
      "source": [
        "joint_documents_set_df = pd.concat([documents_train_df, documents_val_df, documents_test_df])\n",
        "\n",
        "print(\"Length: \", len(joint_documents_set_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_PnpogfjpqS",
        "outputId": "2b494455-4836-4157-faf8-d1da376177dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13:12:53.044 [ForkJoinPool-6-worker-3] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (t24499) - further warnings are suppressed\n",
            "13:12:54.739 [ForkJoinPool-6-worker-3] WARN org.terrier.structures.indexing.Indexer - Indexed 2 empty documents\n"
          ]
        }
      ],
      "source": [
        "indexed = indexer.index(\n",
        "    joint_documents_set_df.to_dict(orient=\"records\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "9xuAPswuVQMe"
      },
      "outputs": [],
      "source": [
        "# Initialize BatchRetrieve with the created index and specify BM25 as the weighting model\n",
        "first_stage_bm25 = pt.BatchRetrieve(\n",
        "    indexed,\n",
        "    wmodel=\"BM25\",\n",
        "    num_results=3,\n",
        "    metadata=[\"docno\", \"vid_name\", \"text\"]\n",
        ")\n",
        "\n",
        "# Initialize BatchRetrieve with the created index and specify LemurTF_IDF as the weighting model\n",
        "first_stage_lemurtfidf = pt.BatchRetrieve(\n",
        "    indexed,\n",
        "    wmodel=\"LemurTF_IDF\",\n",
        "    num_results=2,\n",
        "    metadata=[\"docno\", \"vid_name\", \"text\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vCeCMoUvedO"
      },
      "source": [
        "# Computing feature\n",
        "The weighting model can be use in [pt.weighting_model.package: http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html](http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "QTBZtHYLy_fH"
      },
      "outputs": [],
      "source": [
        "# features, can use any of the features in the list\n",
        "\n",
        "pl2_retriever = pt.BatchRetrieve(indexed, wmodel=\"PL2\")\n",
        "dph_retriever = pt.BatchRetrieve(indexed, wmodel=\"DPH\")\n",
        "tf_idf_retriever = pt.BatchRetrieve(indexed, wmodel=\"TF_IDF\")\n",
        "# bb2_retriever = pt.BatchRetrieve(indexed, wmodel=\"BB2\")\n",
        "# ifb2_retriever = pt.BatchRetrieve(indexed, wmodel=\"IFB2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "AYrqwBx1vVn_"
      },
      "outputs": [],
      "source": [
        "# build a pipeline with the features\n",
        "bm25_pipeline = ~first_stage_bm25 >> (\n",
        "    pl2_retriever ** dph_retriever ** tf_idf_retriever  \n",
        ")\n",
        "\n",
        "#build a pipeline with the features\n",
        "lemurtf_idf_pipeline = ~first_stage_lemurtfidf >> (\n",
        "    pl2_retriever ** dph_retriever ** tf_idf_retriever \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "QE1WrfC8zJjm"
      },
      "outputs": [],
      "source": [
        "# Prepare the queries for the pipeline, remove special characters and extra spaces\n",
        "prepared_trainqueries = queries_train_df\n",
        "prepared_trainqueries['query'] = prepared_trainqueries['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
        "prepared_trainqueries['query'] = prepared_trainqueries['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "prepared_trainqueries['query'] = prepared_trainqueries['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "prepared_train_qrels = pd.DataFrame(query_rankings_data_train)\n",
        "prepared_train_qrels['query'] = prepared_train_qrels['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
        "prepared_train_qrels['query'] = prepared_train_qrels['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "prepared_train_qrels['query'] = prepared_train_qrels['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "prepared_train_qrels['label'] = prepared_train_qrels['score']\n",
        "\n",
        "prepared_train_qrels['label'] = prepared_train_qrels['label'].astype(int)\n",
        "\n",
        "prepared_val_qrels = pd.DataFrame(query_rankings_data_val)\n",
        "prepared_val_qrels['query'] = prepared_val_qrels['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
        "prepared_val_qrels['query'] = prepared_val_qrels['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "prepared_val_qrels['query'] = prepared_val_qrels['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "prepared_val_qrels['label'] = prepared_val_qrels['score']\n",
        "\n",
        "prepared_val_qrels['label'] = prepared_val_qrels['label'].astype(int)\n",
        "\n",
        "prepared_test_qrels = pd.DataFrame(query_rankings_data_test)\n",
        "prepared_test_qrels['query'] = prepared_test_qrels['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
        "prepared_test_qrels['query'] = prepared_test_qrels['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "prepared_test_qrels['query'] = prepared_test_qrels['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "prepared_test_qrels['label'] = prepared_test_qrels['score']\n",
        "\n",
        "prepared_test_qrels['label'] = prepared_test_qrels['label'].astype(int)\n",
        "\n",
        "prepared_qrels = q_rels\n",
        "prepared_qrels['query'] = prepared_qrels['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
        "prepared_qrels['query'] = prepared_qrels['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "prepared_qrels['query'] = prepared_qrels['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "prepared_qrels['label'] = prepared_qrels['score']\n",
        "\n",
        "prepared_qrels['label'] = prepared_qrels['label'].astype(int)\n",
        "\n",
        "prepared_valqueries = queries_val_df.reset_index()\n",
        "prepared_valqueries['query'] = prepared_valqueries['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
        "prepared_valqueries['query'] = prepared_valqueries['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "\n",
        "prepared_valqueries['query'] = prepared_valqueries['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "#test set\n",
        "prepared_testqueries = queries_test_df.reset_index()\n",
        "prepared_testqueries['query'] = prepared_testqueries['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
        "prepared_testqueries['query'] = prepared_testqueries['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "prepared_testqueries['query'] = prepared_testqueries['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLeXGDj8rGcx"
      },
      "source": [
        "Storage format: .csv or trec\n",
        "\n",
        "trec(https://pyterrier.readthedocs.io/en/latest/io.html): The pt io format, but it doesn't contain feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vOyqSXLOlkl",
        "outputId": "b5347def-45d4-47af-94e0-27cc93ea2d7a"
      },
      "outputs": [],
      "source": [
        "# results_with_features = pipeline_with_features(prepared_queries)\n",
        "# results_with_features.to_csv('lemurtf_idf_with_all_features.csv', mode='w', header=True, index=False)\n",
        "# pt.io.write_results(results_with_features, 'lemurtf_idf_with_all_features_ptio')\n",
        "# print(results_with_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "import xgboost as xgb\n",
        "import fastrank\n",
        "\n",
        "\n",
        "index = pt.IndexFactory.of(\"./index_path\")\n",
        "\n",
        "fsr_pipelines = [\n",
        "    {\n",
        "        'pipe': bm25_pipeline,\n",
        "        'name': 'BM25'\n",
        "    },\n",
        "    {\n",
        "        'pipe': lemurtf_idf_pipeline,\n",
        "        'name': 'LemurTF_IDF'\n",
        "    },\n",
        "]\n",
        "\n",
        "learned_models = [\n",
        "    # {\n",
        "    #     'model': SVR(),\n",
        "    #     'form': 'reg',\n",
        "    #     'name': 'SVR',\n",
        "    # },\n",
        "    {\n",
        "        'model': xgb.XGBRanker(tree_method=\"hist\", objective=\"rank:ndcg\"),\n",
        "        'form': 'ltr',\n",
        "        'name': 'XGBoost (NDCG)',\n",
        "     },\n",
        "    # {\n",
        "    #     'model': xgb.XGBRanker(tree_method=\"hist\", objective=\"rank:map\"),\n",
        "    #     'form': 'ltr',\n",
        "    #     'name': 'XGBoost (MAP)',\n",
        "    # },\n",
        "#     {\n",
        "#         'model': fastrank.TrainRequest.coordinate_ascent(),\n",
        "#         'form': 'fastrank',\n",
        "#         'name': 'FastRank Coordinate Ascent',\n",
        "#     },\n",
        "#    {\n",
        "#         'model': fastrank.TrainRequest.random_forest(),\n",
        "#         'form': 'fastrank',\n",
        "#         'name': 'FastRank Random Forest',\n",
        "#     },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BM25 >> XGBoost (NDCG)\n",
            "LemurTF_IDF >> XGBoost (NDCG)\n"
          ]
        }
      ],
      "source": [
        "trained_models = [first_stage_bm25, first_stage_lemurtfidf]\n",
        "names = ['BM25','LemurTF_IDF']\n",
        "\n",
        "for fsr in fsr_pipelines:\n",
        "    for model in learned_models:\n",
        "        names.append(f\"{fsr['name']} >> {model['name']}\")\n",
        "        print(names[-1])\n",
        "        pipe = fsr['pipe'] >> pt.ltr.apply_learned_model(model['model'], form=model['form'])\n",
        "        pipe.fit(\n",
        "            prepared_trainqueries,\n",
        "            prepared_train_qrels,\n",
        "            prepared_valqueries,\n",
        "            prepared_val_qrels\n",
        "        )\n",
        "        trained_models.append(pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>nDCG@1</th>\n",
              "      <th>RR@1</th>\n",
              "      <th>nDCG@3</th>\n",
              "      <th>RR@3</th>\n",
              "      <th>nDCG@5</th>\n",
              "      <th>RR@5</th>\n",
              "      <th>nDCG@10</th>\n",
              "      <th>RR@10</th>\n",
              "      <th>AP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BM25</td>\n",
              "      <td>0.193671</td>\n",
              "      <td>0.193671</td>\n",
              "      <td>0.239239</td>\n",
              "      <td>0.228270</td>\n",
              "      <td>0.239239</td>\n",
              "      <td>0.228270</td>\n",
              "      <td>0.239239</td>\n",
              "      <td>0.228270</td>\n",
              "      <td>0.228270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LemurTF_IDF</td>\n",
              "      <td>0.175949</td>\n",
              "      <td>0.175949</td>\n",
              "      <td>0.207895</td>\n",
              "      <td>0.201266</td>\n",
              "      <td>0.207895</td>\n",
              "      <td>0.201266</td>\n",
              "      <td>0.207895</td>\n",
              "      <td>0.201266</td>\n",
              "      <td>0.201266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BM25 &gt;&gt; XGBoost (NDCG)</td>\n",
              "      <td>0.164557</td>\n",
              "      <td>0.164557</td>\n",
              "      <td>0.226505</td>\n",
              "      <td>0.211181</td>\n",
              "      <td>0.226505</td>\n",
              "      <td>0.211181</td>\n",
              "      <td>0.226505</td>\n",
              "      <td>0.211181</td>\n",
              "      <td>0.211181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LemurTF_IDF &gt;&gt; XGBoost (NDCG)</td>\n",
              "      <td>0.169620</td>\n",
              "      <td>0.169620</td>\n",
              "      <td>0.205559</td>\n",
              "      <td>0.198101</td>\n",
              "      <td>0.205559</td>\n",
              "      <td>0.198101</td>\n",
              "      <td>0.205559</td>\n",
              "      <td>0.198101</td>\n",
              "      <td>0.198101</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            name    nDCG@1      RR@1    nDCG@3      RR@3  \\\n",
              "0                           BM25  0.193671  0.193671  0.239239  0.228270   \n",
              "1                    LemurTF_IDF  0.175949  0.175949  0.207895  0.201266   \n",
              "2         BM25 >> XGBoost (NDCG)  0.164557  0.164557  0.226505  0.211181   \n",
              "3  LemurTF_IDF >> XGBoost (NDCG)  0.169620  0.169620  0.205559  0.198101   \n",
              "\n",
              "     nDCG@5      RR@5   nDCG@10     RR@10        AP  \n",
              "0  0.239239  0.228270  0.239239  0.228270  0.228270  \n",
              "1  0.207895  0.201266  0.207895  0.201266  0.201266  \n",
              "2  0.226505  0.211181  0.226505  0.211181  0.211181  \n",
              "3  0.205559  0.198101  0.205559  0.198101  0.198101  "
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyterrier.measures import nDCG, RR, MAP\n",
        "\n",
        "pt.Experiment(\n",
        "    trained_models,\n",
        "    prepared_testqueries,\n",
        "    prepared_test_qrels,\n",
        "    names=names,\n",
        "    eval_metrics=[nDCG @ 1, RR @ 1, nDCG @ 3, RR @ 3, nDCG @ 5, RR @ 5, nDCG @ 10, RR @ 10, MAP],\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
