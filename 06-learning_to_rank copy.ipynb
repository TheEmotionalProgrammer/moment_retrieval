{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTerrier\n",
    "\n",
    "_IN4325: Information retrieval lecture, TU Delft_\n",
    "\n",
    "**Part 6: Learning to rank**\n",
    "\n",
    "In this part, we'll dive into learning-to-rank (LTR) models. Specifically, we'll cover how to use PyTerrier transformers to\n",
    "\n",
    "- compute query-document features and\n",
    "- train and evaluate LTR models.\n",
    "\n",
    "In order to run everything in this notebook, you'll need [NLTK](https://www.nltk.org/), [scikit-learn](https://scikit-learn.org/), and [LightGBM](https://github.com/microsoft/LightGBM/tree/master/python-package) installed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - python-terrier\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install python-terrier nltk scikit-learn lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init(tqdm=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use NLTK for tokenization later. This requires some data that we need to download first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/isidorotamassia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `nfcorpus` dataset again, as before. In this notebook, we'll use a subset of the queries (`nontopic`). The only reason for this is that it makes the computations faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.get_dataset(\"irds:nfcorpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As LTR models rely on query and document features, we'll include some metadata in our index, namely the titles, abstracts, and URLs.\n",
    "\n",
    "Note that this seems to slow down retrieval quite a bit (even when we're not retrieving the metadata from the index), so this notebook might run slower on your machine than the previous ones. We'll use caching on most transformers to mitigate this problem at least somewhat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/isidorotamassia/Library/Mobile Documents/com~apple~CloudDocs/IR/project/notebooks/06-learning_to_rank.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isidorotamassia/Library/Mobile%20Documents/com~apple~CloudDocs/IR/project/notebooks/06-learning_to_rank.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isidorotamassia/Library/Mobile%20Documents/com~apple~CloudDocs/IR/project/notebooks/06-learning_to_rank.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m index \u001b[39m=\u001b[39m pt\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mIterDictIndexer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isidorotamassia/Library/Mobile%20Documents/com~apple~CloudDocs/IR/project/notebooks/06-learning_to_rank.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mstr\u001b[39m(Path\u001b[39m.\u001b[39mcwd()),  \u001b[39m# this will be ignored\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isidorotamassia/Library/Mobile%20Documents/com~apple~CloudDocs/IR/project/notebooks/06-learning_to_rank.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     meta\u001b[39m=\u001b[39m{\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isidorotamassia/Library/Mobile%20Documents/com~apple~CloudDocs/IR/project/notebooks/06-learning_to_rank.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdocno\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m16\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isidorotamassia/Library/Mobile%20Documents/com~apple~CloudDocs/IR/project/notebooks/06-learning_to_rank.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m256\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isidorotamassia/Library/Mobile%20Documents/com~apple~CloudDocs/IR/project/notebooks/06-learning_to_rank.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m65536\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isidorotamassia/Library/Mobile%20Documents/com~apple~CloudDocs/IR/project/notebooks/06-learning_to_rank.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39murl\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m128\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isidorotamassia/Library/Mobile%20Documents/com~apple~CloudDocs/IR/project/notebooks/06-learning_to_rank.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     },\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isidorotamassia/Library/Mobile%20Documents/com~apple~CloudDocs/IR/project/notebooks/06-learning_to_rank.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mpt\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mIndexingType\u001b[39m.\u001b[39mMEMORY,\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/isidorotamassia/Library/Mobile%20Documents/com~apple~CloudDocs/IR/project/notebooks/06-learning_to_rank.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\u001b[39m.\u001b[39mindex(dataset\u001b[39m.\u001b[39mget_corpus_iter(), fields\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39murl\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/irnbks/lib/python3.11/site-packages/pyterrier/datasets.py:414\u001b[0m, in \u001b[0;36mIRDSDataset.get_corpus_iter\u001b[0;34m(self, verbose, start, count)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[39m# tqdm support\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[0;32m--> 414\u001b[0m     it \u001b[39m=\u001b[39m tqdm(it, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_irds_id\u001b[39m}\u001b[39;00m\u001b[39m documents\u001b[39m\u001b[39m'\u001b[39m, total\u001b[39m=\u001b[39mtotal)\n\u001b[1;32m    416\u001b[0m \u001b[39m# rewrite to follow pyterrier std\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgen\u001b[39m():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/irnbks/lib/python3.11/site-packages/tqdm/notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m unit_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    233\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39m*\u001b[39m unit_scale \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n\u001b[0;32m--> 234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus_printer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp, total, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdesc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mncols)\n\u001b[1;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mpbar \u001b[39m=\u001b[39m proxy(\u001b[39mself\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/irnbks/lib/python3.11/site-packages/tqdm/notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m IProgress \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m total:\n\u001b[1;32m    110\u001b[0m     pbar \u001b[39m=\u001b[39m IProgress(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "index = pt.index.IterDictIndexer(\n",
    "    str(Path.cwd()),  # this will be ignored\n",
    "    meta={\n",
    "        \"docno\": 16,\n",
    "        \"title\": 256,\n",
    "        \"abstract\": 65536,\n",
    "        \"url\": 128,\n",
    "    },\n",
    "    type=pt.index.IndexingType.MEMORY,\n",
    ").index(dataset.get_corpus_iter(), fields=[\"title\", \"abstract\", \"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTR paradigm\n",
    "\n",
    "The idea of learning-to-rank is to use a feature-based supervised machine learning model for ranking. PyTerrier supports end-to-end LTR pipelines, including first-stage retrieval, computation of features, training, and evaluation.\n",
    "\n",
    "### First-stage retrieval\n",
    "\n",
    "LTR models are commonly used in a two-stage process (_retrieve-and-re-rank_): A lightweight retrieval model is used for _candidate selection_ given a query, and the LTR model subsequently _re-ranks_ the candidates. This is because applying the LTR model directly on the whole corpus would be too expensive.\n",
    "\n",
    "We'll use good old BM25 for first-stage retrieval. In order to keep the runtime of this notebook down, we limit the number of documents to be re-ranked to `100`. We also include the metadata of the retrieved documents so we can use it to compute features later:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_stage_bm25 = pt.BatchRetrieve(\n",
    "    index,\n",
    "    wmodel=\"BM25\",\n",
    "    num_results=100,\n",
    "    metadata=[\"docno\", \"title\", \"abstract\", \"url\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing features\n",
    "\n",
    "In order to compute features, we can use PyTerrier transformers. Specifically, the `**` operator (_feature union_) collects features computed by transformers in a designated column in the data frame.\n",
    "\n",
    "In order to illustrate this, we can use other retrievers to compute features. By applying the feature operator, we instruct PyTerrier to use these models for _scoring_ rather than retrieval. Here, we initialize two `pyterrier.BatchRetrieve` objects with the PL2 and DPH weighting models and include them in the pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_features = ~first_stage_bm25 >> (\n",
    "    pt.BatchRetrieve(index, wmodel=\"PL2\") ** pt.BatchRetrieve(index, wmodel=\"DPH\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this pipeline does is the following: For each query,\n",
    "\n",
    "1. retrieve the top-`100` documents using the first-stage retriever (BM25), and\n",
    "2. compute the PL2 and DPH scores for each query-document pair (these are the features).\n",
    "\n",
    "Let's run this pipeline on a single query from the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = pt.get_dataset(\"irds:nfcorpus/test/nontopic\").get_topics()\n",
    "pipeline_with_features(test_queries[test_queries[\"qid\"] == \"PLAIN-102\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row corresponds to one of the candidate documents for this query. The `score` column contains the first-stage retrieval score (BM25), by which the documents are ordered. Finally, the `features` column contains the list of features. In our case, the first feature is the PL2 score, and the second feature is the DPH score.\n",
    "\n",
    "#### Custom features\n",
    "\n",
    "It is also easy to compute our own features. This can be done with custom transformers.\n",
    "\n",
    "Say, for example, we want to compute very simple similarity scores of the query to the title and abstract of each document, respectively. We can do this by implementing a function that takes as input a single row of the data frame (as above) and outputs a list of features (as a `numpy.ndarray`). In our case, we compute the Jaccard similarity of the query to the title and abstract:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def _jaccard_sim(row):\n",
    "    query_tokens = set(nltk.word_tokenize(row[\"query\"].lower()))\n",
    "    title_tokens = set(nltk.word_tokenize(row[\"title\"].lower()))\n",
    "    abstract_tokens = set(nltk.word_tokenize(row[\"abstract\"].lower()))\n",
    "    js_query_title = len(query_tokens & title_tokens) / len(query_tokens | title_tokens)\n",
    "    js_query_abstract = len(query_tokens & abstract_tokens) / len(\n",
    "        query_tokens | abstract_tokens\n",
    "    )\n",
    "    return np.array([js_query_title, js_query_abstract])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Side note: This way of doing it is inefficient, because each query is tokenized multiple times. Can you think of a better way of implementing this?_\n",
    "\n",
    "We can now include this function as a transformer in our pipeline by using [`pyterrier.apply.doc_features`](https://pyterrier.readthedocs.io/en/latest/apply.html#pyterrier.apply.doc_features).\n",
    "\n",
    "You might have noticed that we're accessing the `title` and `abstract` columns in the data frame. This is possible because we added them as metadata during indexing and specified the metadata to be retrieved by `first_stage_bm25`. Alternatively, you can use the [`pyterrier.text.get_text`](https://pyterrier.readthedocs.io/en/latest/text.html#pyterrier.text.get_text) transformer to retrieve metadata from the index.\n",
    "\n",
    "Our new pipeline looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_features = ~first_stage_bm25 >> (\n",
    "    pt.apply.doc_features(_jaccard_sim)\n",
    "    ** pt.BatchRetrieve(index, wmodel=\"PL2\")\n",
    "    ** pt.BatchRetrieve(index, wmodel=\"DPH\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the same query through the new pipeline, we can see that our four features show up in the list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_features(test_queries[test_queries[\"qid\"] == \"PLAIN-102\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to compute a single feature in your custom transformer, you can use [`pyterrier.apply.doc_score`](https://pyterrier.readthedocs.io/en/latest/apply.html#pyterrier.apply.doc_score). Let's add two more features:\n",
    "\n",
    "1. By returning `row[\"score\"]`, we're simply adding the first-stage retrieval score to the feature set.\n",
    "2. We'll also include the length of the URL as a feature.\n",
    "\n",
    "We now have a complete pipeline with six features in total:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_complete = ~first_stage_bm25 >> (\n",
    "    pt.apply.doc_features(_jaccard_sim)\n",
    "    ** pt.BatchRetrieve(index, wmodel=\"PL2\")\n",
    "    ** pt.BatchRetrieve(index, wmodel=\"DPH\")\n",
    "    ** pt.apply.doc_score(lambda row: row[\"score\"])\n",
    "    ** pt.apply.doc_score(lambda row: len(row[\"url\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LTR models\n",
    "\n",
    "The actual models used for re-ranking are not implemented in PyTerrier itself; rather, PyTerrier provides a transformer for trainable models (i.e., regression or LTR models) that implement a scikit-learn-like API (i.e., `fit` and `predict` methods). These trainable transformers are [`pyterrier.Estimator`](https://pyterrier.readthedocs.io/en/latest/transformer.html#pt-transformer-estimator) objects.\n",
    "\n",
    "We'll start by training a simple SVM regression model from scikit-learn. Estimators can be created using [`pyterrier.ltr.apply_learned_model`](https://pyterrier.readthedocs.io/en/latest/ltr.html#pyterrier.ltr.apply_learned_model):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "ltr_svm = ~pipeline_complete >> pt.ltr.apply_learned_model(SVR())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do re-ranking, the model needs to be trained. The `nfcorpus` dataset provides a train/dev/test split, so we can easily load the training data.\n",
    "\n",
    "**Depending on your hardware, the next cell might take a while to execute.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_svm.fit(\n",
    "    pt.get_dataset(\"irds:nfcorpus/train/nontopic\").get_topics(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/train/nontopic\").get_qrels(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly use gradient boosting methods from [XGBoost](https://xgboost.readthedocs.io/en/latest/) and [LightGBM](https://lightgbm.readthedocs.io/en/stable/) by specifying `form=\"ltr\"`. Let's train a [`lightgbm.LGBMRanker`](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRanker.html), which defaults to a LambdaMART model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRanker\n",
    "\n",
    "ltr_lambdamart = ~pipeline_complete >> pt.ltr.apply_learned_model(\n",
    "    LGBMRanker(\n",
    "        metric=\"ndcg\",\n",
    "        importance_type=\"gain\",\n",
    "    ),\n",
    "    form=\"ltr\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model makes use of validation (dev) data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_lambdamart.fit(\n",
    "    pt.get_dataset(\"irds:nfcorpus/train/nontopic\").get_topics(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/train/nontopic\").get_qrels(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/dev/nontopic\").get_topics(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/dev/nontopic\").get_qrels(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the baseline performance (no LTR) with the SVM and LambdaMART models on the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier.measures import nDCG, RR, MAP\n",
    "\n",
    "pt.Experiment(\n",
    "    [first_stage_bm25, ltr_svm, ltr_lambdamart],\n",
    "    pt.get_dataset(\"irds:nfcorpus/test/nontopic\").get_topics(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/test/nontopic\").get_qrels(),\n",
    "    names=[\"BM25\", \"BM25 >> LTR (SVM)\", \"BM25 >> LTR (LambdaMART)\"],\n",
    "    eval_metrics=[nDCG @ 10, RR @ 10, MAP],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeaturesBatchRetrieve\n",
    "\n",
    "So far, we have used the feature union operator (`**`) to append PL2 and DPH scores to our feature list. This is not optimal, because each of the operations requires another index access to compute the features. If we're only interested in those retrieval-based features, we can use [`pyterrier.FeaturesBatchRetrieve`](https://pyterrier.readthedocs.io/en/latest/ltr.html#pyterrier.FeaturesBatchRetrieve) instead, which computes everything at once:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_fbr = pt.FeaturesBatchRetrieve(\n",
    "    index,\n",
    "    wmodel=\"BM25\",\n",
    "    features=[\"WMODEL:BM25\", \"WMODEL:PL2\", \"WMODEL:DPH\"],\n",
    "    num_results=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature ablation\n",
    "\n",
    "Given our approach above (`bm25_fbr`) with three features, we might be interested to know which of these features has the greatest impact on ranking performance. In order to find out, we could create three separate pipelines, where each of them has one of the features removed, and then compare the performance.\n",
    "\n",
    "Luckily, PyTerrier has us covered and provides transformers to make our lives easier: [`pyterrier.ltr.ablate_features`](https://pyterrier.readthedocs.io/en/latest/ltr.html#pyterrier.ltr.ablate_features) can be included in a pipeline to dynamically remove a set of features; [`pyterrier.ltr.keep_features`](https://pyterrier.readthedocs.io/en/latest/ltr.html#pyterrier.ltr.keep_features) does the opposite. Hence, we can simply use it in a loop to get the effect we want:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_lambdamart_abl = {\n",
    "    feature: bm25_fbr\n",
    "    >> pt.ltr.ablate_features(feature)\n",
    "    >> pt.ltr.apply_learned_model(\n",
    "        LGBMRanker(\n",
    "            metric=\"ndcg\",\n",
    "            importance_type=\"gain\",\n",
    "        ),\n",
    "        form=\"ltr\",\n",
    "    )\n",
    "    for feature in [0, 1, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to train each of these models individually:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipeline in ltr_lambdamart_abl.values():\n",
    "    pipeline.fit(\n",
    "        pt.get_dataset(\"irds:nfcorpus/train/nontopic\").get_topics(),\n",
    "        pt.get_dataset(\"irds:nfcorpus/train/nontopic\").get_qrels(),\n",
    "        pt.get_dataset(\"irds:nfcorpus/dev/nontopic\").get_topics(),\n",
    "        pt.get_dataset(\"irds:nfcorpus/dev/nontopic\").get_qrels(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines, names = [], []\n",
    "for feature, pipeline in ltr_lambdamart_abl.items():\n",
    "    pipelines.append(pipeline)\n",
    "    names.append(f\"LambdaMART (feature {feature} removed)\")\n",
    "\n",
    "pt.Experiment(\n",
    "    pipelines,\n",
    "    pt.get_dataset(\"irds:nfcorpus/test/nontopic\").get_topics(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/test/nontopic\").get_qrels(),\n",
    "    names=names,\n",
    "    eval_metrics=[nDCG @ 10, RR @ 10, MAP],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "Check out the [LTR section](https://pyterrier.readthedocs.io/en/latest/ltr.html) in the PyTerrier documentation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
